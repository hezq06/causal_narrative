{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558a2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# %matplotlib\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b43a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_narrative.fmriutil import *\n",
    "from causal_narrative.datautil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b4ebe",
   "metadata": {},
   "source": [
    "## Prepare Tokenization for each story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487e7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load from  /work1/hezq17/causal_narrative/workspace/notebook/../../dataset/narratives/code/task_meta.json\n",
      "['21styear', 'black', 'bronx', 'forgot', 'lucy', 'merlin', 'milkyway', 'notthefallintact', 'notthefalllongscram', 'notthefallshortscram', 'pieman', 'piemanpni', 'prettymouth', 'schema', 'shapesphysical', 'shapessocial', 'sherlock', 'slumlordreach', 'tunnel']\n",
      "['notthefalllongscram', 'notthefallshortscram', 'schema']\n",
      "Task notthefalllongscram removed\n",
      "Task notthefallshortscram removed\n",
      "Task schema removed\n",
      "Data load from  /work1/hezq17/causal_narrative/workspace/notebook/../../dataset/narratives/code/scan_exclude.json\n"
     ]
    }
   ],
   "source": [
    "ndp = NarrativeDataPreprocess({\"opt_model\":\"opt_no_case\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95495224",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndp.tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7647a",
   "metadata": {},
   "source": [
    "## Prepare alignment matrix for each story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndp.gen_alignment_mats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9c379",
   "metadata": {},
   "source": [
    "## Do Opt encoding with each opt and each story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_ls = [\"opt-125m\", \"opt-350m\",\"opt-1.3b\",\"opt-2.7b\",\"opt-6.7b\"]\n",
    "opt_ls = [\"opt-125m_no_case\",\"opt-350m\"]\n",
    "# opt_ls = [\"opt-6.7b\"]\n",
    "# opt_ls = [\"t5\"]\n",
    "ndp = NarrativeDataPreprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt_model in opt_ls:\n",
    "    print(\"Current opt model,\",opt_model)\n",
    "    for task in ndp.tasks:\n",
    "        if task not in ndp.tasks_exclude:\n",
    "            futil = FmriUtil({\"opt_model\": opt_model})\n",
    "            gptcodeall = futil.opt_encode(window=128, task=task, cuda_device=\"cuda:0\") #<6.7b\n",
    "#             gptcodeall = futil.opt_encode_batch(window=256, task=task, chuck_size=64, cuda_device=\"cuda:0\") #<6.7b\n",
    "#             gptcodeall = futil.clip_encode_att(window=75, task=task, cuda_device=\"cuda:0\")\n",
    "#             gptcodeall = futil.t5_encode_block(window=76, task=task, cuda_device=\"cuda:0\") #<6.7b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:causal] *",
   "language": "python",
   "name": "conda-env-causal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
